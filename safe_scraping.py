"""
GÃ¼venli ve Etik Web Scraping YardÄ±mcÄ± FonksiyonlarÄ±
Proxy olmadan gÃ¼venli scraping iÃ§in gerekli araÃ§lar
"""

import time
import random
import requests
from urllib.robotparser import RobotFileParser
from urllib.parse import urljoin, urlparse
import logging
from typing import Optional, Dict, List
import config

class SafeScrapingManager:
    """GÃ¼venli scraping iÃ§in yÃ¶netici sÄ±nÄ±f"""
    
    def __init__(self):
        self.request_times = {}  # Site bazÄ±nda istek zamanlarÄ±
        self.robots_cache = {}  # Robots.txt cache
        self.domain_user_agents = {}  # Domain baÅŸÄ±na User-Agent
        self.session = requests.Session()
        self.setup_session()
    
    def setup_session(self):
        """GÃ¼venli session ayarlarÄ±"""
        # Rastgele User-Agent seÃ§
        self.session.headers.update({
            'User-Agent': random.choice(config.USER_AGENTS),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
    
    def check_robots_txt(self, url: str, user_agent: str = '*') -> bool:
        """GeliÅŸmiÅŸ Robots.txt kontrolÃ¼ - Cache ile"""
        try:
            parsed_url = urlparse(url)
            domain = parsed_url.netloc
            robots_url = f"{parsed_url.scheme}://{domain}/robots.txt"

            # Cache kontrolÃ¼
            if domain not in self.robots_cache:
                print(f"ğŸ¤– Robots.txt kontrol ediliyor: {domain}")
                rp = RobotFileParser()
                rp.set_url(robots_url)
                rp.read()
                self.robots_cache[domain] = rp
                print(f"âœ… Robots.txt cache'lendi: {domain}")
            else:
                rp = self.robots_cache[domain]

            # Ä°zin kontrolÃ¼
            allowed = rp.can_fetch(user_agent, url)
            if not allowed:
                print(f"ğŸš« Robots.txt tarafÄ±ndan yasaklandÄ±: {url}")
            else:
                print(f"âœ… Robots.txt izin veriyor: {url}")

            return allowed

        except Exception as e:
            logging.warning(f"Robots.txt kontrolÃ¼ baÅŸarÄ±sÄ±z ({domain}): {e}")
            print(f"âš ï¸  Robots.txt hatasÄ±, izin veriliyor: {domain}")
            return True  # Hata durumunda izin ver
    
    def get_domain_rate_limits(self, domain: str) -> Dict:
        """Domain iÃ§in rate limit ayarlarÄ±nÄ± getir"""
        return config.RATE_LIMITS.get(domain, config.RATE_LIMITS['default'])

    def rate_limit_check(self, domain: str) -> bool:
        """GeliÅŸmiÅŸ rate limiting kontrolÃ¼ - Domain bazÄ±nda"""
        current_time = time.time()

        if domain not in self.request_times:
            self.request_times[domain] = []

        # Domain iÃ§in Ã¶zel ayarlarÄ± al
        limits = self.get_domain_rate_limits(domain)

        # Son 1 dakikadaki istekleri filtrele
        minute_ago = current_time - 60
        recent_requests = [
            req_time for req_time in self.request_times[domain]
            if req_time > minute_ago
        ]

        # Son 1 saatteki istekleri filtrele
        hour_ago = current_time - 3600
        hourly_requests = [
            req_time for req_time in self.request_times[domain]
            if req_time > hour_ago
        ]

        # DakikalÄ±k limit kontrolÃ¼
        if len(recent_requests) >= limits['requests_per_minute']:
            print(f"â±ï¸  Rate limit (dakika): {domain} - {len(recent_requests)}/{limits['requests_per_minute']}")
            return False

        # Saatlik limit kontrolÃ¼
        if len(hourly_requests) >= limits['requests_per_hour']:
            print(f"â±ï¸  Rate limit (saat): {domain} - {len(hourly_requests)}/{limits['requests_per_hour']}")
            return False

        return True
    
    def get_domain_user_agent(self, domain: str) -> str:
        """Domain iÃ§in tutarlÄ± User-Agent getir veya yeni ata"""
        if config.USER_AGENT_CHANGE_PER_DOMAIN:
            if domain not in self.domain_user_agents:
                self.domain_user_agents[domain] = random.choice(config.USER_AGENTS)
                print(f"ğŸ”„ Yeni User-Agent atandÄ±: {domain}")
            return self.domain_user_agents[domain]
        else:
            return random.choice(config.USER_AGENTS)

    def smart_delay(self, domain: str):
        """GeliÅŸmiÅŸ akÄ±llÄ± gecikme sistemi - Domain bazÄ±nda"""
        if domain not in self.request_times:
            self.request_times[domain] = []

        current_time = time.time()
        limits = self.get_domain_rate_limits(domain)

        # Son istek zamanÄ±nÄ± kontrol et
        if self.request_times[domain]:
            last_request = max(self.request_times[domain])
            time_since_last = current_time - last_request

            # Domain iÃ§in Ã¶zel gecikme sÃ¼resi
            min_delay = random.uniform(limits['min_delay'], limits['max_delay'])
            if time_since_last < min_delay:
                sleep_time = min_delay - time_since_last
                print(f"â±ï¸  AkÄ±llÄ± gecikme ({domain}): {sleep_time:.2f} saniye bekleniyor...")
                time.sleep(sleep_time)

        # Ä°stek zamanÄ±nÄ± kaydet
        self.request_times[domain].append(time.time())
    
    def safe_get(self, url: str, **kwargs) -> Optional[requests.Response]:
        """GÃ¼venli GET isteÄŸi"""
        try:
            # Domain Ã§Ä±kar
            domain = urlparse(url).netloc
            
            # Robots.txt kontrolÃ¼
            if not self.check_robots_txt(url):
                logging.warning(f"Robots.txt tarafÄ±ndan yasaklandÄ±: {url}")
                return None
            
            # Rate limiting kontrolÃ¼
            if not self.rate_limit_check(domain):
                logging.warning(f"Rate limit aÅŸÄ±ldÄ±: {domain}")
                return None
            
            # AkÄ±llÄ± gecikme
            self.smart_delay(domain)
            
            # GeliÅŸmiÅŸ User-Agent rotation
            if random.random() < config.USER_AGENT_ROTATION_CHANCE:
                new_ua = self.get_domain_user_agent(domain)
                self.session.headers['User-Agent'] = new_ua
                print(f"ğŸ”„ User-Agent deÄŸiÅŸtirildi: {domain}")
            else:
                # Domain iÃ§in mevcut UA'yÄ± kullan
                self.session.headers['User-Agent'] = self.get_domain_user_agent(domain)
            
            # Ä°steÄŸi gÃ¶nder
            response = self.session.get(
                url, 
                timeout=config.REQUEST_TIMEOUT,
                **kwargs
            )
            
            # BaÅŸarÄ± durumunu logla
            logging.info(f"âœ… BaÅŸarÄ±lÄ± istek: {url} (Status: {response.status_code})")
            return response
            
        except requests.exceptions.RequestException as e:
            logging.error(f"âŒ Ä°stek hatasÄ±: {url} - {e}")
            return None
        except Exception as e:
            logging.error(f"âŒ Beklenmeyen hata: {url} - {e}")
            return None
    
    def get_safe_headers(self) -> Dict[str, str]:
        """GÃ¼venli header'lar dÃ¶ndÃ¼r"""
        return {
            'User-Agent': random.choice(config.USER_AGENTS),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Cache-Control': 'max-age=0',
        }

# Global instance
safe_scraper = SafeScrapingManager()

def get_safe_session() -> SafeScrapingManager:
    """GÃ¼venli scraping session'Ä± dÃ¶ndÃ¼r"""
    return safe_scraper

def is_scraping_allowed(url: str) -> bool:
    """URL'nin scraping iÃ§in gÃ¼venli olup olmadÄ±ÄŸÄ±nÄ± kontrol et"""
    return safe_scraper.check_robots_txt(url)

def safe_request(url: str, **kwargs) -> Optional[requests.Response]:
    """GÃ¼venli HTTP isteÄŸi wrapper'Ä±"""
    return safe_scraper.safe_get(url, **kwargs)
